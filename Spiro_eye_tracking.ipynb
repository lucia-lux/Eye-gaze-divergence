{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of eye tracking files from Spiro study.\n",
    "Gaze dispersion metric based on Christoforou et al, 2015: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4428128/pdf/fpsyg-06-00579.pdf\n",
    "The objective is as follows:\n",
    "(1) compute within-subject dispersion metric, based on short sections of the film (250 ms with 50 ms shift, ie 80% overlap)\n",
    "(2) get sections with extreme dispersion score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse film data, v3 Oct/Nov 2020\n",
    "\n",
    "# read and prep Tobii .tsv files from CortEx study\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import nanmedian, nanmean, nanstd\n",
    "from scipy.spatial import distance\n",
    "from scipy import stats\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "# get participant numbers\n",
    "\n",
    "def get_pnums(infiles):\n",
    "    \"\"\"\n",
    "    Get participant numbers from filenames.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    infiles: array of input file names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of participant numbers \n",
    "    \"\"\"\n",
    "    id = []\n",
    "    for i,filenames in enumerate(infiles):\n",
    "        pnum = [int(s) for s in filenames.split('_') if s.isdigit()]\n",
    "        id.append([pnum])\n",
    "\n",
    "def flatten(list_name):\n",
    "    \"\"\" \n",
    "    Function that flattens list of lists.\n",
    "    Source: https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-a-list-of-lists\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_name: str\n",
    "        Name of the list to flatten\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Flattened list\n",
    "    \"\"\"\n",
    "    flattened = [item for sublist in list_name for item in sublist]\n",
    "    return flattened\n",
    "\n",
    "\n",
    "\n",
    "def get_random_samps(df, frame_num_max, id):\n",
    "    \"\"\" \n",
    "    Fit a participant's gaze data to normal distribution.\n",
    "    Randomly sample normal distribution with participant-specific mean and std.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "        dataframe holding eye tracking data from all participants\n",
    "\n",
    "    frame_num_max: int\n",
    "        maximum number of frames for participant, will be used to determine how many samples to draw\n",
    "\n",
    "    id: int\n",
    "        participant id \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Simulated gaze data in x and y.\n",
    "\n",
    "    \"\"\"\n",
    "    # first, let's simulate some gaze data.\n",
    "    # some basic parameters\n",
    "    #gaze_length = df.frame_num.max() # number of samples we need to generate\n",
    "    mux,stdx = norm.fit(df.loc[df.id == id,'gaze_in_x'])\n",
    "    muy,stdy = norm.fit(df.loc[df.id == id, 'gaze_in_y'])\n",
    "    # get samples\n",
    "    sim_in_x = norm.rvs(loc = mux, scale = stdx, size = frame_num_max)\n",
    "    sim_in_y = norm.rvs(loc = muy, scale = stdy, size = frame_num_max)\n",
    "    return sim_in_x, sim_in_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tobii_file:\n",
    "    def __init__(self, filename, film_dur_s, screenres_x = None, screenres_y = None, s_rate= None):\n",
    "        \"\"\"\n",
    "    \n",
    "        \"\"\"\n",
    "        if not screenres_x:\n",
    "            self.screen_resx = 1280\n",
    "        if not screenres_y:\n",
    "            self.screen_resy = 1024\n",
    "        if not s_rate:\n",
    "            s_rate = 60\n",
    "        self.filename = filename\n",
    "        self.screen_res = [screenres_x, screenres_y]\n",
    "        self.s_rate = s_rate\n",
    "        self.film_dur_s = film_dur_s\n",
    "        self.pnum = [int(s) for s in filename.split('_') if s.isdigit()]\n",
    "\n",
    "    def get_calibration(self, calibration_filename, calibration_dir):\n",
    "        \"\"\"\n",
    "        Get calibration details (accuracy, precision).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename: str\n",
    "            Calibration filename\n",
    "        calibration file directory: str\n",
    "            path to calibration file directory\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        self.calibration_filename = calibration_filename\n",
    "        calibration_df = pd.read_csv(os.path.join(calibration_dir,calibration_filename))\n",
    "        if calibration_df.loc[calibration_df['used']=='used','used'].empty:\n",
    "            self.calibration = 'unused'\n",
    "        else:\n",
    "            self.calibration = calibration_df.loc[calibration_df['used'] == 'used',:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) get time stamps from message file\n",
    "- discard everything but the messages containing frame nr.\n",
    "(ii) load data file and add column for frame nr\n",
    "(iii) label rows between message (framenr) time stamps with the appropriate frame nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = 1 \n",
    "if home:\n",
    "    rawfilepath = r\"C:\\Users\\Luzia T\\Eye-gaze-divergence\"\n",
    "    Tobii_files = [f for f in os.listdir(rawfilepath) if 'merged' in f]\n",
    "    calibration_files = [f for f in os.listdir(rawfilepath) if 'calib' in f]\n",
    "else:\n",
    "    # first we need to establish where to find the files and read them in.\n",
    "    rawfilepath = r\"P:\\Spironolactone\\eye_tracking\\Tobii\"\n",
    "    # get merged Tobii files - these have both event info and eye gaze data.\n",
    "    Tobii_files = [f for f in os.listdir(rawfilepath) if 'merged' in f]\n",
    "    # get_calibration files\n",
    "    calibration_files = [f for f in os.listdir(rawfilepath) if 'calib' in f and f.endswith('.tsv')]\n",
    "    msg_files = [f for f in os.listdir(rawfilepath) if 'msg' in f and f.endswith('.tsv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pnum, left/right gaze point on display area, event messages \n",
    "# keep only columns between framenum 0 and the last frame\n",
    "# set invalid samples to NaN (validity == 0)\n",
    "frames = []\n",
    "gaze_in_x = []\n",
    "gaze_in_y = []\n",
    "pnums = []\n",
    "\n",
    "for i,filename in enumerate(Tobii_files):\n",
    "    # read file\n",
    "    tobii_file = pd.read_table(os.path.join(rawfilepath, filename))\n",
    "    # get participant number and add as column\n",
    "    tobii_file['pnum'] = np.repeat([int(s) for s in filename.split('_') if s.isdigit()], tobii_file.shape[0])\n",
    "    # get row index of frame each frame presentation\n",
    "    event_index = tobii_file.loc[tobii_file.msg.str.contains('FRAME',na = False),'msg'].index\n",
    "    # drop everything before first frame index (ie frame 0) and after last frame\n",
    "    tobii_file = tobii_file.loc[event_index[0]:event_index[-1],:]\n",
    "    # extract frame number\n",
    "    tobii_file.loc[event_index, 'msg'] = [f[0] for f in tobii_file.loc[event_index,'msg'].str.split(';', n = 1)]\n",
    "    tobii_file['frame_num'] = tobii_file.loc[event_index,'msg'].apply(lambda x: re.findall(r'\\d',x)).apply(''.join).astype('int')\n",
    "    # fill the rows between event markers with the appropriate frame number\n",
    "    tobii_file.loc[:,'frame_num'] = tobii_file.loc[:,'frame_num'].fillna(method = 'ffill')\n",
    "    # drop event markers\n",
    "    tobii_file = tobii_file.drop(labels = event_index,axis = 0)\n",
    "    # discard invalid samples\n",
    "    tobii_file = tobii_file.loc[(tobii_file.right_gaze_point_validity == 1)&(tobii_file.left_gaze_point_validity == 1),:]\n",
    "    # group by frame number and calculate mean\n",
    "    tobii_file = tobii_file.groupby('frame_num').mean()\n",
    "    tobii_file = tobii_file.reset_index()\n",
    "    # collect data into lists\n",
    "    pnums.append(tobii_file.pnum.values)\n",
    "    gaze_in_x.append(tobii_file.right_gaze_point_on_display_area_x.values)\n",
    "    gaze_in_y.append(tobii_file.left_gaze_point_on_display_area_y.values)\n",
    "    frames.append(tobii_file.frame_num.values)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create one big dataframe from the above.\n",
    "This is already ~ half a million rows for only 24 participants (with max 1 row per frame). This is going to become unwieldy for a significantly larger number of participants...may need to think about restructuring/parallel computing/cloud use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "et_df = pd.DataFrame({'id':flatten(pnums),'gaze_in_x':flatten(gaze_in_x),'gaze_in_y':flatten(gaze_in_y),'frame_num':flatten(frames)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have valid data for left/right gaze positions (on display area) for all participants in a single data frame. We now want to get a measure of divergence. We need to bear in mind a couple of things.\n",
    "First of all, calibration accuracy/precision will be different between participants. The first thing to check is to make sure that the precision is roughly the same. If not, this could really affect the divergence measure in a fairly unpredictable way, so we would need to exclude those subjects. Next, we want to have a look at accuracy. This is less of an issue because it is basically just an offset. We could try to correct gaze point data for each participant, but this will not be needed as divergent frames are identified based on comparing to a 'random' sequence of gaze data (which we will simulate below). Unless the offset is extreme, this should therefore not affect the results.\n",
    "Because different participants are missing different frames and because we don't expect huge variation between individual frames, given a frame rate of ~30 fps, we will look at 250 ms windows, and we will use a sliding window approach, shifting the window 50 ms each time (resulting in 80% overlap between windows).\n",
    "For each window and participant, we will calculate the Euclidean distance between the participant in question and every other participant/the participant in question and the random sequence. For any given window, we will then check whether this distance is greater in the former case than in the latter. If so, the window is identified as 'divergent' for that participant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's simulate x/y data by sampling from a normal distribution, based on each participant's gaze data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.6 ms ± 2.71 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# create columns to hold simulated values\n",
    "et_df['gaze_in_x_sim'] = np.nan\n",
    "et_df['gaze_in_y_sim'] = np.nan\n",
    "\n",
    "# fill with simulated gaze data in x and y for each participant\n",
    "for i,pnum in enumerate(et_df.id.unique()):\n",
    "    pnum_df = et_df.loc[et_df.id == pnum,['id','gaze_in_x','gaze_in_y']]\n",
    "    max_frame = pnum_df.shape[0]\n",
    "    simx,simy = get_random_samps(pnum_df,max_frame,pnum)\n",
    "    et_df.loc[et_df.id == pnum, 'gaze_in_x_sim'] = simx\n",
    "    et_df.loc[et_df.id == pnum, 'gaze_in_y_sim'] = simy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's construct a dataframe for the x and y gaze positions that holds data from all participants.\n",
    "gaze_x = et_df.pivot(index = 'frame_num',columns = 'id', values = 'gaze_in_x')\n",
    "gaze_y = et_df.pivot(index = 'frame_num',columns = 'id', values = 'gaze_in_y')\n",
    "gaze_x_sim = et_df.pivot(index = 'frame_num',columns = 'id', values = 'gaze_in_x_sim')\n",
    "gaze_y_sim = et_df.pivot(index = 'frame_num',columns = 'id', values = 'gaze_in_y_sim')\n",
    "\n",
    "#et_df.loc[:,['frame_num','gaze_in_x','gaze_in_y']].groupby('frame_num').rolling(window = 15, min_periods = 3).apply(distance.euclidean)\n",
    "dists = []\n",
    "for i in gaze_y.index:\n",
    "    frame_df = pd.concat([gaze_x.loc[i,:],gaze_y.loc[i,:]],axis = 1)\n",
    "    frame_dist_val =  np.unique(pairwise_distances(frame_df.dropna(), frame_df.dropna(),force_all_finite = True)).mean()\n",
    "    dists.append(frame_dist_val)\n",
    "\n",
    "# now we calculate the mean euclidean distance for each window.\n",
    "pwise_dists = pd.Series(dists, index = gaze_x.index)\n",
    "windows = pwise_dists.rolling(window = 15)\n",
    "steps = np.arange(0,len(dists)-15,step = 3)\n",
    "\n",
    "# now calculate the mean for each 15-frame window (corresponds to ~250 ms), with a step size of 3 (80% overlap between successive windows)\n",
    "wind_dists = []\n",
    "for i in steps:\n",
    "    wind_dist = np.array(dists)[i:i+15].mean()\n",
    "    wind_dists.append(wind_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we calculate the distance between each individual's gaze location for each frame and the 'random' gaze positions we generated above by sampling from a normal distribution.\n",
    "\n",
    "# calculcate Euclidean distance\n",
    "dist_sim_df = np.sqrt(np.square(gaze_x-gaze_x_sim)+np.square(gaze_y-gaze_y_sim)).mean(axis=1)\n",
    "\n",
    "# now calculate mean for each window. Can use the steps from above as the arrays are the same length.\n",
    "wind_dists_sim = []\n",
    "for i in steps:\n",
    "    wind_dist_sim = dist_sim_df.dropna().iloc[i:i+15].mean()\n",
    "    wind_dists_sim.append(wind_dist_sim)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_act_sim = pd.Series(wind_dists)-pd.Series(wind_dists_sim)\n",
    "# where the difference between actual and simulated dispersion scores is >0, window is divergent\n",
    "divergent_winds = diff_act_sim[diff_act_sim>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of divergent windows is: 28.065494238932686\n"
     ]
    }
   ],
   "source": [
    "# %age of divergent windows:\n",
    "perc_div_winds = (len(divergent_winds)/len(diff_act_sim))*100\n",
    "print(\"The percentage of divergent windows is:\", perc_div_winds)\n",
    "\n",
    "inds_div_winds = divergent_winds.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get consecutive sections of divergent windows.This is so we can relate it to sections of the film.\n",
    "divergent_winds = divergent_winds.reset_index()\n",
    "divergent_winds = divergent_winds.rename(columns = {'index':'div_winds'})\n",
    "divwinds_shifted = divergent_winds.shift(-1).reset_index(drop = True)\n",
    "divwinds_shifted = divwinds_shifted.rename(columns = {'index':'div_winds'})\n",
    "consecs = divergent_winds.div_winds - divwinds_shifted.div_winds\n",
    "\n",
    "consec_inds = consecs[consecs != -1].index\n",
    "divergent_winds_consec = divergent_winds.drop(labels = consec_inds,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "## option #2 to get the consecutive indices. \n",
    "vals = divergent_winds.index\n",
    "consecs = []\n",
    "for i in range(0,len(vals)-1):\n",
    "    if vals[i]+1==vals[i+1]:\n",
    "        consecs.append(vals[i])\n",
    "    else:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48186e61764c8c514947f0ef500accf59797b98e64cdc910e21ec2975c1f1025"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
